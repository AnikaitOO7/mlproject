Setup & Environment Configuration
Create a GitHub Repository:

Initialize the project repository:

git init
git remote add origin https://github.com/AnikaitOO7/mlproject.git
Create a README.md to describe the project, and push the initial commit:

git add README.md  
git commit -m "Initial Commit"  
git branch -M main  
git push -u origin main  
Create a Virtual Environment:


conda create -p venv python==3.8 -y  # '-y' auto-approves installation
conda activate ./venv/
Clone the Repository & Sync with GitHub:

Use GitHub to ensure code versioning and collaboration.
Create .gitignore File:

Select Python template to exclude unnecessary files like .pyc and virtual environments.
Create setup.py and requirements.txt:

setup.py is used to build and distribute the application as a package.
Use -e . in requirements.txt to run the project in editable mode.

pip install -r requirements.txt
Project Structure & Python Packages:

In src/ directory, create __init__.py to mark it as a package.
Use find_packages() in setup.py to ensure internal folders are included as packages.
Project Structure, Logging, Exception Handling, and Utilities
Create the Core Project Structure in src/:


src/
├── components/         # Custom modules (data ingestion, transformation, etc.)
│   ├── __init__.py     
│   ├── data_ingestion.py
│   ├── data_transformation.py
│   └── model_trainer.py
├── pipeline/           # Train and predict pipelines
│   ├── __init__.py
│   ├── train_pipeline.py
│   └── predict_pipeline.py
├── logger.py           # Logging configuration
├── exception.py        # Custom exceptions
└── utils.py            # Reusable utility functions
Set Up Custom Logging & Exception Handling:

In logger.py, configure logs with timestamps and industry-standard formats.
In exception.py, define a custom exception class to handle project-specific errors.
Data Ingestion & Transformation:

data_ingestion.py: Loads data from databases/files and splits it into train/test sets.
data_transformation.py: Prepares data by encoding categorical features (using OneHotEncoding/Label Encoding) and scaling numerical data.
Model Training & Evaluation (model_trainer.py):

Trains multiple models and evaluates them using R², MSE, and MAE.
Implements pipelines for both train and predict phases.
Project with Deployment: EDA and Model Training Workflow
Problem Statement:
Analyze how various factors (gender, parental education, etc.) affect students' math scores.

Create a notebook/ Folder:
Place the dataset (StudentsPerformance.csv) in a data/ folder.
Create two notebooks:
EDA_Student_Performance.ipynb: For exploratory data analysis.
Model_Training.ipynb: For training and evaluating models.
Exploratory Data Analysis (EDA):
Understand the Problem Statement and Data:

The dataset consists of 8 columns and 1000 rows.
Target feature: Math score.
Perform Data Checks:

Check for missing values, duplicates, data types, and unique categories.
Visualize score distributions using histograms and KDE plots.
Conclusions from EDA:

Performance correlates with lunch type, parental education, and gender.
Female students tend to have higher scores overall.
Model Training & Hyperparameter Tuning
Install Required Libraries:

pip install scikit-learn xgboost catboost
Model Training Overview:

Train using models such as:
Linear Regression, RandomForest, AdaBoost, XGBoost, CatBoost, Ridge, Lasso, SVR.
Use RandomizedSearchCV for hyperparameter tuning.
Evaluate Models Using Metrics:

Metrics:
R² Score
Mean Squared Error (MSE)
Mean Absolute Error (MAE)
Store the best-performing model as model.pkl.
Feature Preparation:

X: All features except for math score.
Y: math score.
CI/CD Pipeline Using GitHub Actions
Set Up CI/CD Pipeline:

Create a .github/workflows/ci.yml file to automate:
Linting and Testing: Run pytest and check code formatting.
Model Training: Retrain models upon code changes.
Artifact Upload: Automatically save the best model (model.pkl) to GitHub.
CI/CD Workflow Example:

yaml
name: CI/CD Pipeline

on: [push, pull_request]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.8

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run tests
        run: |
          pytest

      - name: Save model artifact
        run: |
          python src/train_pipeline.py
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
MLOps Best Practices
Version Control:

Store all artifacts (trained models, preprocessor files) in GitHub for reproducibility.
Automated Testing & Deployment:

Use GitHub Actions to run tests and deploy the best model automatically.
Model Monitoring:

Track model performance with logging and metrics to ensure consistent performance.
